{"id":"bench-4gk","title":"Stage all benchmark datasets","description":"Download, convert, and stage all benchmark datasets for use across database adapters.\n\n## Datasets to Stage\n\n### OLTP Datasets (1MB - 50GB)\n- [ ] E-commerce (products, orders, customers, reviews)\n- [ ] SaaS multi-tenant (orgs, users, workspaces, resources)\n- [ ] Social network (users, posts, comments, relationships)\n- [ ] IoT time-series (devices, readings, aggregates)\n\n### Analytics Datasets\n- [ ] ClickBench (99M rows, 14GB)\n- [ ] Wiktionary (~10GB, full-text)\n- [ ] Wikidata (100M+ entities, subset: 1M/10M/100M)\n- [ ] Common Crawl Host Graph (~50GB)\n\n### Reference Datasets\n- [ ] IMDb (3-4GB: titles, names, ratings, principals)\n- [ ] FDA FAERS (15-20GB: adverse events, drugs, reactions)\n- [ ] Open Food Facts (4GB: products, nutrition, barcodes)\n\n### Financial Datasets\n- [ ] Banking ledger (accounts, transfers, statements)\n- [ ] E-commerce payments (merchants, orders, settlements)\n- [ ] General ledger (chart of accounts, journal entries)\n\n### Standards Datasets (from graph.org.ai / standards.org.ai)\n- [ ] O*NET (75K+ occupation entities)\n- [ ] UNSPSC (180K+ product entities)\n- [ ] BLS Wages (6M+ records)\n- [ ] GeoNames (11M+ places)\n- [ ] NAICS/APQC crosswalks (340K+ mappings)\n\n## Output Format\n\nEach dataset should produce:\n1. Raw download in `datasets/raw/{name}/`\n2. Parquet files in `datasets/parquet/{name}/` (for DuckDB/analytics)\n3. SQLite seed database in `datasets/sqlite/{name}.db`\n4. JSON/JSONL for document stores in `datasets/jsonl/{name}/`\n5. R2 upload manifest for lakehouse pattern\n\n## Size Tiers\n\nGenerate multiple size tiers for scaling benchmarks:\n- `1mb`, `10mb`, `100mb`, `1gb`, `10gb`, `20gb`, `30gb`, `50gb`\n\n## Acceptance Criteria\n\n- [ ] All datasets have download scripts in `scripts/download/`\n- [ ] All datasets have conversion scripts in `scripts/convert/`\n- [ ] All datasets have verification checksums\n- [ ] README documents dataset sources, licenses, and schemas\n- [ ] CI can regenerate any dataset from scratch","status":"open","priority":0,"issue_type":"epic","created_at":"2026-01-21T06:14:31.428657-06:00","updated_at":"2026-01-21T12:16:22Z","external_ref":"gh-1"}
{"id":"bench-4gk.1","title":"Create dataset download scripts","description":"Create download scripts for all datasets in `scripts/download/`.\n\nScripts needed:\n- `download-clickbench.sh` - ClickBench TSV from GitHub\n- `download-imdb.sh` - IMDb datasets from datasets.imdbws.com\n- `download-fda-faers.sh` - FDA quarterly data files\n- `download-openfoodfacts.sh` - Open Food Facts CSV dump\n- `download-geonames.sh` - GeoNames allCountries.txt\n- `download-onet.sh` - O*NET database files\n- `download-wiktionary.sh` - Wiktionary XML dump\n- `download-wikidata.sh` - Wikidata JSON dump (subset)\n\nEach script should:\n- Check if already downloaded (skip if exists)\n- Verify checksums after download\n- Handle rate limiting / retries\n- Support partial downloads (curl -C -)","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-21T06:15:15.655579-06:00","updated_at":"2026-01-21T06:15:15.655579-06:00","external_ref":"gh-2","dependencies":[{"issue_id":"bench-4gk.1","depends_on_id":"bench-4gk","type":"parent-child","created_at":"2026-01-21T06:15:15.65627-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-4gk.2","title":"Create dataset conversion scripts","description":"Create conversion scripts to transform raw downloads into benchmark-ready formats.\n\nScripts needed in `scripts/convert/`:\n- `convert-to-parquet.ts` - Convert CSV/TSV to Parquet (for DuckDB)\n- `convert-to-sqlite.ts` - Load into SQLite seed database\n- `convert-to-jsonl.ts` - Convert to JSONL (for document stores)\n- `convert-to-tigerbeetle.ts` - Convert financial data to TB format\n\nFeatures:\n- Size tier generation (1mb, 10mb, 100mb, 1gb, etc.)\n- Schema validation\n- Foreign key preservation\n- Index recommendations per database","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-21T06:15:17.368528-06:00","updated_at":"2026-01-21T06:15:17.368528-06:00","external_ref":"gh-3","dependencies":[{"issue_id":"bench-4gk.2","depends_on_id":"bench-4gk","type":"parent-child","created_at":"2026-01-21T06:15:17.369074-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-4gk.3","title":"Stage OLTP datasets (e-commerce, SaaS, social, IoT)","description":"Generate synthetic OLTP datasets at multiple size tiers.\n\nUse faker or similar to generate realistic:\n- E-commerce: products, orders, customers, reviews, inventory\n- SaaS: orgs, users, workspaces, documents, audit logs\n- Social: users, posts, comments, likes, follows, messages\n- IoT: devices, sensors, readings, alerts, aggregates\n\nSize tiers: 1mb, 10mb, 100mb, 1gb, 10gb, 20gb, 30gb, 50gb\n\nOutput formats:\n- SQLite seed database\n- JSONL for document stores\n- Parquet for analytics\n- SQL INSERT scripts","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-21T06:15:18.501021-06:00","updated_at":"2026-01-21T12:16:24Z","external_ref":"gh-4"}
{"id":"bench-4gk.4","title":"Stage analytics datasets (ClickBench, IMDb, FDA)","description":"Download and stage large analytics datasets.\n\nDatasets:\n- ClickBench: 99M rows web analytics (hits table)\n- IMDb: titles, names, ratings, principals (~4GB)\n- FDA FAERS: adverse events, drugs, reactions (~20GB)\n- GeoNames: 11M+ places (~2GB)\n- Open Food Facts: 4M+ products (~4GB)\n\nRequirements:\n- Verify download integrity\n- Create schema documentation\n- Generate Parquet for DuckDB\n- Create subsets for quick testing (1M rows)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-21T06:15:19.691436-06:00","updated_at":"2026-01-21T12:16:25Z","external_ref":"gh-5"}
{"id":"bench-9zg","title":"Run comprehensive database benchmarks and generate reports","description":"Run all databases against all compatible datasets and generate comprehensive benchmark reports.\n\n## Database × Dataset Compatibility Matrix\n\n| Dataset | db4 | evodb | postgres | sqlite | duckdb | tigerbeetle | graphdb | sdb | @db4/mongo | @dotdo/mongodb |\n|---------|-----|-------|----------|--------|--------|-------------|---------|-----|------------|----------------|\n| E-commerce OLTP | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ | ✓ | ✓ |\n| SaaS multi-tenant | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ | ✓ | ✓ |\n| Social network | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ | ✓ | ✓ | ✓ | ✓ |\n| IoT time-series | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ | ✓ |\n| ClickBench | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ | ✗ | ✗ | ✗ | ✗ |\n| Wiktionary | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ | ✗ | ✗ | ✗ | ✗ |\n| Wikidata | ✗ | ✗ | ✓ | ✗ | ✓ | ✗ | ✓ | ✓ | ✗ | ✓ |\n| IMDb | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ | ✓ | ✓ | ✓ | ✓ |\n| FDA FAERS | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ | ✗ | ✗ | ✗ | ✓ |\n| Open Food Facts | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ | ✓ | ✓ | ✓ |\n| Banking ledger | ✗ | ✗ | ✓ | ✓ | ✗ | ✓ | ✗ | ✗ | ✗ | ✗ |\n| O*NET | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ | ✓ | ✓ | ✓ | ✓ |\n| GeoNames | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ | ✗ | ✗ | ✗ | ✓ |\n\n## Benchmark Categories\n\n### 1. OLTP Benchmarks\n- Point lookups (by PK)\n- Range scans (indexed columns)\n- Inserts (single row, batch 100, batch 1000)\n- Updates (single row, batch)\n- Deletes\n- Transactions (multi-statement)\n\n### 2. Analytics Benchmarks (DuckDB/Postgres/SQLite)\n- Full table scans\n- Aggregations (COUNT, SUM, AVG, GROUP BY)\n- Complex JOINs (3+ tables)\n- Window functions\n- ClickBench 43 queries\n\n### 3. Graph Benchmarks (GraphDB/SDB)\n- 1-hop traversal\n- 2-hop traversal\n- Path queries\n- Reverse traversal (backlinks)\n- Pattern matching\n\n### 4. Financial Benchmarks (TigerBeetle)\n- Account creation throughput\n- Transfer processing TPS\n- Balance lookups\n- Two-phase transfers\n\n### 5. Document Benchmarks (MongoDB compat)\n- Find by field\n- Find with projection\n- Aggregation pipeline\n- Bulk inserts\n\n## Output Format\n\n### Raw Results: JSONL\n```jsonl\n{\"benchmark\":\"oltp/point-lookup\",\"database\":\"db4\",\"dataset\":\"ecommerce-1gb\",\"p50_ms\":0.12,\"p99_ms\":0.45,\"ops_per_sec\":8500,\"timestamp\":\"2024-01-21T12:00:00Z\"}\n{\"benchmark\":\"oltp/point-lookup\",\"database\":\"postgres\",\"dataset\":\"ecommerce-1gb\",\"p50_ms\":0.18,\"p99_ms\":0.72,\"ops_per_sec\":5600,\"timestamp\":\"2024-01-21T12:00:00Z\"}\n```\n\n### Processed Report: Markdown Tables\n```markdown\n## OLTP Point Lookups (ecommerce-1gb)\n\n| Database | P50 (ms) | P99 (ms) | ops/sec | Cost/1M |\n|----------|----------|----------|---------|---------|\n| db4      | 0.12     | 0.45     | 8,500   | $0.001  |\n| postgres | 0.18     | 0.72     | 5,600   | $0.001  |\n| sqlite   | 0.15     | 0.58     | 6,800   | $0.001  |\n```\n\n## Report Generation\n\n1. **Per-database reports**: `results/{database}/report.md`\n2. **Per-dataset reports**: `results/{dataset}/comparison.md`\n3. **Summary report**: `results/BENCHMARK_REPORT.md`\n4. **Raw data**: `results/raw/*.jsonl`\n5. **Charts**: `results/charts/*.svg` (optional, using mermaid)\n\n## Issues to Highlight\n\n- [ ] Memory usage at each size tier\n- [ ] Cold start latency (WASM load time)\n- [ ] VFS actual blob read/write counts\n- [ ] Cost per 1M operations\n- [ ] Bundle size impact\n- [ ] Missing query support per database\n\n## Acceptance Criteria\n\n- [ ] All compatible database×dataset combinations run\n- [ ] JSONL output for every benchmark run\n- [ ] Markdown report generator in `scripts/generate-report.ts`\n- [ ] CI can run full benchmark suite\n- [ ] Known issues documented per database\n- [ ] Cost analysis based on actual VFS metrics","status":"open","priority":0,"issue_type":"epic","created_at":"2026-01-21T06:14:57.505438-06:00","updated_at":"2026-01-21T06:14:57.505438-06:00"}
{"id":"bench-9zg.1","title":"Create JSONL benchmark output format","description":"Define and implement JSONL output format for benchmark results.\n\nSchema:\n```typescript\ninterface BenchmarkResult {\n  // Identification\n  benchmark: string        // e.g., \"oltp/point-lookup\"\n  database: string         // e.g., \"db4\", \"postgres\"\n  dataset: string          // e.g., \"ecommerce-1gb\"\n  \n  // Timing\n  p50_ms: number\n  p95_ms: number\n  p99_ms: number\n  min_ms: number\n  max_ms: number\n  mean_ms: number\n  \n  // Throughput\n  ops_per_sec: number\n  iterations: number\n  \n  // Cost metrics\n  rows_read: number\n  rows_written: number\n  estimated_cost_per_1m: number\n  \n  // Metadata\n  timestamp: string        // ISO 8601\n  git_sha: string\n  environment: string      // \"local\" | \"ci\" | \"cloudflare\"\n  \n  // Optional\n  error?: string\n  notes?: string\n}\n```\n\nImplementation:\n- Create `lib/benchmark-output.ts` with writer class\n- Integrate with vitest bench hooks\n- Output to `results/raw/{database}-{dataset}-{timestamp}.jsonl`","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-21T06:15:52.073625-06:00","updated_at":"2026-01-21T12:16:26Z","external_ref":"gh-7"}
{"id":"bench-9zg.2","title":"Create markdown report generator","description":"Create script to generate markdown reports from JSONL results.\n\nScript: `scripts/generate-report.ts`\n\nFeatures:\n- Parse all JSONL files from `results/raw/`\n- Group by benchmark category, database, dataset\n- Generate comparison tables\n- Calculate cost estimates\n- Highlight winners/losers\n- Flag known issues\n\nOutput files:\n- `results/BENCHMARK_REPORT.md` - Full summary\n- `results/by-database/{database}.md` - Per-database details\n- `results/by-dataset/{dataset}.md` - Per-dataset comparisons\n- `results/by-category/{category}.md` - OLTP, analytics, graph, etc.\n\nTable format:\n```markdown\n| Database | P50 | P99 | ops/sec | Cost/1M | Notes |\n|----------|-----|-----|---------|---------|-------|\n| db4      | 0.12| 0.45| 8,500   | $0.001  | ✓     |\n| postgres | 0.18| 0.72| 5,600   | $0.001  | WASM  |\n```","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-21T06:15:53.505184-06:00","updated_at":"2026-01-21T06:15:53.505184-06:00","dependencies":[{"issue_id":"bench-9zg.2","depends_on_id":"bench-9zg","type":"parent-child","created_at":"2026-01-21T06:15:53.506144-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-9zg.3","title":"Run OLTP benchmarks across all databases","description":"Run OLTP benchmark suite against compatible databases.\n\nDatabases: db4, evodb, postgres, sqlite, @db4/mongo, @dotdo/mongodb, sdb\nDatasets: ecommerce, saas, social (at 1mb, 10mb, 100mb, 1gb tiers)\n\nBenchmarks:\n- Point lookup by PK\n- Range scan (10, 100, 1000 rows)\n- Single insert\n- Batch insert (100, 1000 rows)\n- Single update\n- Batch update\n- Delete\n- Transaction (read-modify-write)\n\nRun command: `pnpm bench:oltp --output-jsonl`","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-21T06:15:55.115287-06:00","updated_at":"2026-01-21T06:15:55.115287-06:00","external_ref":"gh-9","dependencies":[{"issue_id":"bench-9zg.3","depends_on_id":"bench-9zg","type":"parent-child","created_at":"2026-01-21T06:15:55.115877-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-9zg.4","title":"Run analytics benchmarks (ClickBench suite)","description":"Run ClickBench 43 queries against analytics databases.\n\nDatabases: postgres, sqlite, duckdb\nDataset: ClickBench hits table (99M rows)\n\nQueries: All 43 official ClickBench queries\n- Q1-Q10: Simple aggregations\n- Q11-Q20: GROUP BY queries\n- Q21-Q30: Complex filters\n- Q31-Q43: JOINs and window functions\n\nCompare with official ClickBench results.\nOutput per-query timings in JSONL.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-21T06:15:56.820249-06:00","updated_at":"2026-01-21T12:16:28Z","external_ref":"gh-10"}
{"id":"bench-9zg.5","title":"Run graph benchmarks (GraphDB, SDB, db4)","description":"Run graph traversal benchmarks.\n\nDatabases: graphdb, sdb, db4\nDatasets: social network, O*NET occupation graph, Wikidata subset\n\nBenchmarks:\n- 1-hop traversal (friends, skills, related entities)\n- 2-hop traversal (friends of friends)\n- Path queries (A -> B with constraints)\n- Reverse traversal (who links to X?)\n- Pattern matching (find all Type with property=value)\n- Batch entity retrieval (100 entities)\n\nOutput traversal depth vs latency curves.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-21T06:15:58.173472-06:00","updated_at":"2026-01-21T12:16:28Z","external_ref":"gh-11"}
{"id":"bench-9zg.6","title":"Run financial benchmarks (TigerBeetle)","description":"Run financial/accounting benchmarks.\n\nDatabase: TigerBeetle WASM\nDatasets: banking, ecommerce-payments, general-ledger\n\nBenchmarks:\n- Account creation throughput (target: 10K/sec)\n- Transfer processing TPS (target: 100K/sec)\n- Balance lookups\n- Two-phase commit transfers\n- Batch operations (8190 batch size limit)\n- Account history queries\n\nCompare with in-memory db4/sqlite implementations.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-21T06:15:59.534559-06:00","updated_at":"2026-01-21T12:16:29Z","external_ref":"gh-12"}
{"id":"bench-9zg.7","title":"Document known issues and limitations per database","description":"Create `KNOWN_ISSUES.md` documenting database-specific limitations.\n\nPer database, document:\n- Missing SQL features (CTEs, window functions, etc.)\n- Query limitations (no JOINs, max result size, etc.)\n- Type limitations (bigint, JSON, arrays, etc.)\n- Performance gotchas (cold start, memory limits)\n- Cost considerations (blob read/write patterns)\n- WASM-specific issues (bundle size, load time)\n\nFormat:\n```markdown\n## db4\n- ✗ No SQL support (document store only)\n- ✗ No JOINs (must denormalize or multi-query)\n- ✓ Native JSON/document support\n- ⚠ 2MB blob limit per write\n\n## postgres (PGlite WASM)\n- ✓ Full SQL support\n- ✓ JOINs, CTEs, window functions\n- ⚠ 13-14 MB WASM bundle\n- ⚠ Cold start ~500ms\n```","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-21T06:16:00.918084-06:00","updated_at":"2026-01-21T06:16:00.918084-06:00","dependencies":[{"issue_id":"bench-9zg.7","depends_on_id":"bench-9zg","type":"parent-child","created_at":"2026-01-21T06:16:00.918699-06:00","created_by":"Nathan Clevenger"}]}