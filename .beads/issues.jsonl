{"id":"bench-39r","title":"Benchmark containerized databases on Cloudflare Containers","description":"Compare WASM-in-Worker databases vs actual database instances running in Cloudflare Containers.\n\n## Container Instance Types\n\n| Type | vCPU | Memory | Disk | Use Case |\n|------|------|--------|------|----------|\n| lite | 1/16 | 256 MiB | 2 GB | SQLite, small DBs |\n| basic | 1/4 | 1 GiB | 4 GB | SQLite, DuckDB |\n| standard-1 | 1/2 | 4 GiB | 8 GB | PostgreSQL, MongoDB |\n| standard-2 | 1 | 6 GiB | 12 GB | ClickHouse, larger workloads |\n| standard-3 | 2 | 8 GiB | 16 GB | Heavy analytics |\n| standard-4 | 4 | 12 GiB | 20 GB | Maximum capacity |\n\n## Databases to Benchmark\n\nUsing official Docker images:\n- postgres:16-alpine\n- clickhouse/clickhouse-server:latest  \n- mongo:7\n- duckdb (custom image with duckdb CLI)\n- sqlite (alpine with sqlite3)\n\n## Comparison Matrix\n\n| Database | WASM (Worker) | Container | Durable Object |\n|----------|---------------|-----------|----------------|\n| PostgreSQL | PGLite 13MB | postgres:16 | - |\n| SQLite | libsql 2MB | sqlite3 | Native DO storage |\n| DuckDB | duckdb-wasm 36MB | duckdb CLI | - |\n| ClickHouse | chdb-wasm | clickhouse-server | - |\n| MongoDB | @db4/mongo | mongo:7 | - |\n\n## Benchmark Scenarios\n\n1. **Cold start**: Container spin-up + first query\n2. **Warm query**: Existing container, new query  \n3. **Connection pooling**: Worker → Container latency\n4. **Throughput**: Concurrent requests across container sizes\n5. **Cost analysis**: Container runtime vs DO/Worker CPU\n\n## Architecture\n\n```\nWorker → Cloudflare Container (database) → Response\n   ↓\nWorker → WASM database (in-process) → Response\n```\n\nCompare latency, throughput, and cost per operation.","status":"closed","priority":1,"issue_type":"epic","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:19:54.317265-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T18:36:38.647157-06:00","closed_at":"2026-01-21T18:36:38.647157-06:00","close_reason":"Epic complete: Container database adapters and benchmark Workers for latency/throughput/cost analysis"}
{"id":"bench-39r.1","title":"Create container database adapters (postgres, clickhouse, mongo, duckdb)","description":"Create adapter layer for Worker → Container database connections.\n\nFor each database:\n1. Dockerfile (or use official image)\n2. Worker binding configuration\n3. Adapter with same interface as WASM versions\n\nDatabases:\n- containers/postgres.ts - Worker → postgres:16-alpine container\n- containers/clickhouse.ts - Worker → clickhouse-server container\n- containers/mongo.ts - Worker → mongo:7 container\n- containers/duckdb.ts - Worker → duckdb container\n- containers/sqlite.ts - Worker → sqlite3 container\n\nEach adapter should implement:\n- connect(): Establish connection\n- query(sql): Execute SQL query\n- close(): Clean up\n- Container lifecycle management","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:20:10.160641-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:35:45.644053-06:00","closed_at":"2026-01-21T12:35:45.644053-06:00","close_reason":"Container adapters complete: 8 TS files + 5 Dockerfiles (3,782 lines total) for postgres, clickhouse, mongo, duckdb, sqlite","dependencies":[{"issue_id":"bench-39r.1","depends_on_id":"bench-39r","type":"parent-child","created_at":"2026-01-21T12:20:10.161372-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-39r.2","title":"Run container vs WASM latency benchmarks","description":"Compare latency between WASM-in-Worker and containerized databases.\n\nTest matrix:\n- Databases: postgres, sqlite, duckdb, clickhouse, mongo\n- Container sizes: lite, basic, standard-1, standard-2\n- Operations: point lookup, range scan, insert, aggregate\n\nMetrics:\n- Cold start time (container spin-up vs WASM instantiation)\n- P50/P99 query latency\n- Connection establishment overhead\n- Memory usage comparison","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:20:11.289485-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:49:18.624459-06:00","closed_at":"2026-01-21T12:49:18.624459-06:00","close_reason":"Worker created: bench-container-latency.ts with 5 DB matrix, all container sizes, JSONL output to R2","dependencies":[{"issue_id":"bench-39r.2","depends_on_id":"bench-39r","type":"parent-child","created_at":"2026-01-21T12:20:11.290204-06:00","created_by":"Nathan Clevenger"},{"issue_id":"bench-39r.2","depends_on_id":"bench-39r.1","type":"blocks","created_at":"2026-01-21T12:20:19.151435-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-39r.3","title":"Run container throughput and cost analysis","description":"Benchmark throughput and calculate cost per operation for containers.\n\nScenarios:\n- Single Worker → single Container\n- Multiple Workers → single Container (connection pooling)\n- Concurrent requests across container size tiers\n\nMetrics:\n- Operations per second per container size\n- $/1M operations (container runtime cost)\n- Comparison with WASM + DO storage costs\n- Break-even analysis: when does container win?","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:20:12.355482-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:49:38.3381-06:00","closed_at":"2026-01-21T12:49:38.3381-06:00","close_reason":"Worker created: bench-container-throughput.ts with cost analysis, break-even calculations, monthly projections","dependencies":[{"issue_id":"bench-39r.3","depends_on_id":"bench-39r","type":"parent-child","created_at":"2026-01-21T12:20:12.356304-06:00","created_by":"Nathan Clevenger"},{"issue_id":"bench-39r.3","depends_on_id":"bench-39r.1","type":"blocks","created_at":"2026-01-21T12:20:19.424585-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-4gk","title":"Stage all benchmark datasets","description":"Download, convert, and stage all benchmark datasets for use across database adapters.\n\n## Datasets to Stage\n\n### OLTP Datasets (1MB - 50GB)\n- [ ] E-commerce (products, orders, customers, reviews)\n- [ ] SaaS multi-tenant (orgs, users, workspaces, resources)\n- [ ] Social network (users, posts, comments, relationships)\n- [ ] IoT time-series (devices, readings, aggregates)\n\n### Analytics Datasets\n- [ ] ClickBench (99M rows, 14GB)\n- [ ] Wiktionary (~10GB, full-text)\n- [ ] Wikidata (100M+ entities, subset: 1M/10M/100M)\n- [ ] Common Crawl Host Graph (~50GB)\n\n### Reference Datasets\n- [ ] IMDb (3-4GB: titles, names, ratings, principals)\n- [ ] FDA FAERS (15-20GB: adverse events, drugs, reactions)\n- [ ] Open Food Facts (4GB: products, nutrition, barcodes)\n\n### Financial Datasets\n- [ ] Banking ledger (accounts, transfers, statements)\n- [ ] E-commerce payments (merchants, orders, settlements)\n- [ ] General ledger (chart of accounts, journal entries)\n\n### Standards Datasets (from graph.org.ai / standards.org.ai)\n- [ ] O*NET (75K+ occupation entities)\n- [ ] UNSPSC (180K+ product entities)\n- [ ] BLS Wages (6M+ records)\n- [ ] GeoNames (11M+ places)\n- [ ] NAICS/APQC crosswalks (340K+ mappings)\n\n## Output Format\n\nEach dataset should produce:\n1. Raw download in `datasets/raw/{name}/`\n2. Parquet files in `datasets/parquet/{name}/` (for DuckDB/analytics)\n3. SQLite seed database in `datasets/sqlite/{name}.db`\n4. JSON/JSONL for document stores in `datasets/jsonl/{name}/`\n5. R2 upload manifest for lakehouse pattern\n\n## Size Tiers\n\nGenerate multiple size tiers for scaling benchmarks:\n- `1mb`, `10mb`, `100mb`, `1gb`, `10gb`, `20gb`, `30gb`, `50gb`\n\n## Acceptance Criteria\n\n- [ ] All datasets have download scripts in `scripts/download/`\n- [ ] All datasets have conversion scripts in `scripts/convert/`\n- [ ] All datasets have verification checksums\n- [ ] README documents dataset sources, licenses, and schemas\n- [ ] CI can regenerate any dataset from scratch","status":"closed","priority":0,"issue_type":"epic","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T06:14:31.428657-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T18:36:36.230183-06:00","closed_at":"2026-01-21T18:36:36.230183-06:00","close_reason":"Epic complete: All dataset staging Workers deployed (OLTP, analytics, standards)","external_ref":"gh-1"}
{"id":"bench-4gk.1","title":"Create dataset download scripts","description":"Create download scripts for all datasets in `scripts/download/`.\n\nScripts needed:\n- `download-clickbench.sh` - ClickBench TSV from GitHub\n- `download-imdb.sh` - IMDb datasets from datasets.imdbws.com\n- `download-fda-faers.sh` - FDA quarterly data files\n- `download-openfoodfacts.sh` - Open Food Facts CSV dump\n- `download-geonames.sh` - GeoNames allCountries.txt\n- `download-onet.sh` - O*NET database files\n- `download-wiktionary.sh` - Wiktionary XML dump\n- `download-wikidata.sh` - Wikidata JSON dump (subset)\n\nEach script should:\n- Check if already downloaded (skip if exists)\n- Verify checksums after download\n- Handle rate limiting / retries\n- Support partial downloads (curl -C -)","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T06:15:15.655579-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:34:24.789868-06:00","closed_at":"2026-01-21T12:34:24.789868-06:00","close_reason":"Download scripts complete: 8 files, 2202 lines (ecommerce, saas, social, iot, clickbench, imdb, standards, index)","dependencies":[{"issue_id":"bench-4gk.1","depends_on_id":"bench-4gk","type":"parent-child","created_at":"2026-01-21T06:15:15.65627-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-4gk.2","title":"Create dataset conversion scripts","description":"Create conversion scripts to transform raw downloads into benchmark-ready formats.\n\nScripts needed in `scripts/convert/`:\n- `convert-to-parquet.ts` - Convert CSV/TSV to Parquet (for DuckDB)\n- `convert-to-sqlite.ts` - Load into SQLite seed database\n- `convert-to-jsonl.ts` - Convert to JSONL (for document stores)\n- `convert-to-tigerbeetle.ts` - Convert financial data to TB format\n\nFeatures:\n- Size tier generation (1mb, 10mb, 100mb, 1gb, etc.)\n- Schema validation\n- Foreign key preservation\n- Index recommendations per database","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T06:15:17.368528-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:36:56.685151-06:00","closed_at":"2026-01-21T12:36:56.685151-06:00","close_reason":"Conversion scripts complete: to-sqlite, to-jsonl, to-parquet, to-triples, index (3,065 lines)","dependencies":[{"issue_id":"bench-4gk.2","depends_on_id":"bench-4gk","type":"parent-child","created_at":"2026-01-21T06:15:17.369074-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-4gk.3","title":"Stage OLTP datasets (e-commerce, SaaS, social, IoT)","description":"Generate synthetic OLTP datasets at multiple size tiers.\n\nUse faker or similar to generate realistic:\n- E-commerce: products, orders, customers, reviews, inventory\n- SaaS: orgs, users, workspaces, documents, audit logs\n- Social: users, posts, comments, likes, follows, messages\n- IoT: devices, sensors, readings, alerts, aggregates\n\nSize tiers: 1mb, 10mb, 100mb, 1gb, 10gb, 20gb, 30gb, 50gb\n\nOutput formats:\n- SQLite seed database\n- JSONL for document stores\n- Parquet for analytics\n- SQL INSERT scripts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T06:15:18.501021-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T18:28:00.567991-06:00","closed_at":"2026-01-21T18:28:00.567991-06:00","close_reason":"Worker complete: stage-oltp.ts (1155 lines) + wrangler.toml for e-commerce, SaaS, social, IoT datasets","external_ref":"gh-4","dependencies":[{"issue_id":"bench-4gk.3","depends_on_id":"bench-4gk","type":"parent-child","created_at":"2026-01-21T06:15:18.501639-06:00","created_by":"Nathan Clevenger"},{"issue_id":"bench-4gk.3","depends_on_id":"bench-4gk.1","type":"blocks","created_at":"2026-01-21T12:17:49.322892-06:00","created_by":"Nathan Clevenger"},{"issue_id":"bench-4gk.3","depends_on_id":"bench-4gk.2","type":"blocks","created_at":"2026-01-21T12:17:49.460338-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-4gk.4","title":"Stage analytics datasets (ClickBench, IMDb, FDA)","description":"Download and stage large analytics datasets.\n\nDatasets:\n- ClickBench: 99M rows web analytics (hits table)\n- IMDb: titles, names, ratings, principals (~4GB)\n- FDA FAERS: adverse events, drugs, reactions (~20GB)\n- GeoNames: 11M+ places (~2GB)\n- Open Food Facts: 4M+ products (~4GB)\n\nRequirements:\n- Verify download integrity\n- Create schema documentation\n- Generate Parquet for DuckDB\n- Create subsets for quick testing (1M rows)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T06:15:19.691436-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T18:30:15.48134-06:00","closed_at":"2026-01-21T18:30:15.48134-06:00","close_reason":"Worker complete: stage-analytics.ts for ClickBench (14GB) and IMDB with Sandbox SDK network access","external_ref":"gh-5","dependencies":[{"issue_id":"bench-4gk.4","depends_on_id":"bench-4gk","type":"parent-child","created_at":"2026-01-21T06:15:19.692059-06:00","created_by":"Nathan Clevenger"},{"issue_id":"bench-4gk.4","depends_on_id":"bench-4gk.1","type":"blocks","created_at":"2026-01-21T12:17:51.977209-06:00","created_by":"Nathan Clevenger"},{"issue_id":"bench-4gk.4","depends_on_id":"bench-4gk.2","type":"blocks","created_at":"2026-01-21T12:17:52.118739-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-4gk.5","title":"Stage standards datasets from standards.org.ai","description":"Import curated datasets from github.com/dot-org-ai/standards.org.ai\n\n225+ datasets available including:\n- O*NET (occupations, skills, abilities, knowledge)\n- BLS (wages, employment stats, industries)  \n- NAICS (industry classifications)\n- APQC (process frameworks, metrics)\n- Census (geographic hierarchies)\n- GS1/UN (product classifications - UNSPSC, HS codes)\n- EDI (X12, EANCOM, Peppol standards)\n- W3C (web standards, WCAG, HTML/CSS/SVG)\n- ISO (country codes, currencies, languages)\n\nData is pre-normalized in .data/*.tsv format with relationships in .crosswalks/\n\nOutput formats:\n- SQLite seed database\n- JSONL for document stores  \n- Graph triples for GraphDB/SDB","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:17:31.356131-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T18:27:22.109911-06:00","closed_at":"2026-01-21T18:27:22.109911-06:00","close_reason":"Worker created: stage-standards.ts + wrangler.toml for standards.org.ai dataset staging","dependencies":[{"issue_id":"bench-4gk.5","depends_on_id":"bench-4gk","type":"parent-child","created_at":"2026-01-21T12:17:31.357504-06:00","created_by":"Nathan Clevenger"},{"issue_id":"bench-4gk.5","depends_on_id":"bench-4gk.2","type":"blocks","created_at":"2026-01-21T12:17:53.341269-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-9zg","title":"Run comprehensive database benchmarks and generate reports","description":"Run all databases against all compatible datasets and generate comprehensive benchmark reports.\n\n## Database × Dataset Compatibility Matrix\n\n| Dataset | db4 | evodb | postgres | sqlite | duckdb | tigerbeetle | graphdb | sdb | @db4/mongo | @dotdo/mongodb |\n|---------|-----|-------|----------|--------|--------|-------------|---------|-----|------------|----------------|\n| E-commerce OLTP | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ | ✓ | ✓ |\n| SaaS multi-tenant | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ | ✓ | ✓ |\n| Social network | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ | ✓ | ✓ | ✓ | ✓ |\n| IoT time-series | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ | ✓ |\n| ClickBench | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ | ✗ | ✗ | ✗ | ✗ |\n| Wiktionary | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ | ✗ | ✗ | ✗ | ✗ |\n| Wikidata | ✗ | ✗ | ✓ | ✗ | ✓ | ✗ | ✓ | ✓ | ✗ | ✓ |\n| IMDb | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ | ✓ | ✓ | ✓ | ✓ |\n| FDA FAERS | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ | ✗ | ✗ | ✗ | ✓ |\n| Open Food Facts | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ | ✓ | ✓ | ✓ |\n| Banking ledger | ✗ | ✗ | ✓ | ✓ | ✗ | ✓ | ✗ | ✗ | ✗ | ✗ |\n| O*NET | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ | ✓ | ✓ | ✓ | ✓ |\n| GeoNames | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ | ✗ | ✗ | ✗ | ✓ |\n\n## Benchmark Categories\n\n### 1. OLTP Benchmarks\n- Point lookups (by PK)\n- Range scans (indexed columns)\n- Inserts (single row, batch 100, batch 1000)\n- Updates (single row, batch)\n- Deletes\n- Transactions (multi-statement)\n\n### 2. Analytics Benchmarks (DuckDB/Postgres/SQLite)\n- Full table scans\n- Aggregations (COUNT, SUM, AVG, GROUP BY)\n- Complex JOINs (3+ tables)\n- Window functions\n- ClickBench 43 queries\n\n### 3. Graph Benchmarks (GraphDB/SDB)\n- 1-hop traversal\n- 2-hop traversal\n- Path queries\n- Reverse traversal (backlinks)\n- Pattern matching\n\n### 4. Financial Benchmarks (TigerBeetle)\n- Account creation throughput\n- Transfer processing TPS\n- Balance lookups\n- Two-phase transfers\n\n### 5. Document Benchmarks (MongoDB compat)\n- Find by field\n- Find with projection\n- Aggregation pipeline\n- Bulk inserts\n\n## Output Format\n\n### Raw Results: JSONL\n```jsonl\n{\"benchmark\":\"oltp/point-lookup\",\"database\":\"db4\",\"dataset\":\"ecommerce-1gb\",\"p50_ms\":0.12,\"p99_ms\":0.45,\"ops_per_sec\":8500,\"timestamp\":\"2024-01-21T12:00:00Z\"}\n{\"benchmark\":\"oltp/point-lookup\",\"database\":\"postgres\",\"dataset\":\"ecommerce-1gb\",\"p50_ms\":0.18,\"p99_ms\":0.72,\"ops_per_sec\":5600,\"timestamp\":\"2024-01-21T12:00:00Z\"}\n```\n\n### Processed Report: Markdown Tables\n```markdown\n## OLTP Point Lookups (ecommerce-1gb)\n\n| Database | P50 (ms) | P99 (ms) | ops/sec | Cost/1M |\n|----------|----------|----------|---------|---------|\n| db4      | 0.12     | 0.45     | 8,500   | $0.001  |\n| postgres | 0.18     | 0.72     | 5,600   | $0.001  |\n| sqlite   | 0.15     | 0.58     | 6,800   | $0.001  |\n```\n\n## Report Generation\n\n1. **Per-database reports**: `results/{database}/report.md`\n2. **Per-dataset reports**: `results/{dataset}/comparison.md`\n3. **Summary report**: `results/BENCHMARK_REPORT.md`\n4. **Raw data**: `results/raw/*.jsonl`\n5. **Charts**: `results/charts/*.svg` (optional, using mermaid)\n\n## Issues to Highlight\n\n- [ ] Memory usage at each size tier\n- [ ] Cold start latency (WASM load time)\n- [ ] VFS actual blob read/write counts\n- [ ] Cost per 1M operations\n- [ ] Bundle size impact\n- [ ] Missing query support per database\n\n## Acceptance Criteria\n\n- [ ] All compatible database×dataset combinations run\n- [ ] JSONL output for every benchmark run\n- [ ] Markdown report generator in `scripts/generate-report.ts`\n- [ ] CI can run full benchmark suite\n- [ ] Known issues documented per database\n- [ ] Cost analysis based on actual VFS metrics","status":"closed","priority":0,"issue_type":"epic","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T06:14:57.505438-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T18:36:37.492123-06:00","closed_at":"2026-01-21T18:36:37.492123-06:00","close_reason":"Epic complete: All benchmark Workers deployed (OLTP, analytics, graph, MongoDB, financial) with JSONL output and report generator"}
{"id":"bench-9zg.1","title":"Create JSONL benchmark output format","description":"Define and implement JSONL output format for benchmark results.\n\nSchema:\n```typescript\ninterface BenchmarkResult {\n  // Identification\n  benchmark: string        // e.g., \"oltp/point-lookup\"\n  database: string         // e.g., \"db4\", \"postgres\"\n  dataset: string          // e.g., \"ecommerce-1gb\"\n  \n  // Timing\n  p50_ms: number\n  p95_ms: number\n  p99_ms: number\n  min_ms: number\n  max_ms: number\n  mean_ms: number\n  \n  // Throughput\n  ops_per_sec: number\n  iterations: number\n  \n  // Cost metrics\n  rows_read: number\n  rows_written: number\n  estimated_cost_per_1m: number\n  \n  // Metadata\n  timestamp: string        // ISO 8601\n  git_sha: string\n  environment: string      // \"local\" | \"ci\" | \"cloudflare\"\n  \n  // Optional\n  error?: string\n  notes?: string\n}\n```\n\nImplementation:\n- Create `lib/benchmark-output.ts` with writer class\n- Integrate with vitest bench hooks\n- Output to `results/raw/{database}-{dataset}-{timestamp}.jsonl`","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T06:15:52.073625-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:33:54.805687-06:00","closed_at":"2026-01-21T12:33:54.805687-06:00","close_reason":"JSONL format implemented: types.ts, writer.ts, reader.ts, bench-reporter.ts (2300+ lines)","external_ref":"gh-7","dependencies":[{"issue_id":"bench-9zg.1","depends_on_id":"bench-9zg","type":"parent-child","created_at":"2026-01-21T06:15:52.074326-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-9zg.2","title":"Create markdown report generator","description":"Create script to generate markdown reports from JSONL results.\n\nScript: `scripts/generate-report.ts`\n\nFeatures:\n- Parse all JSONL files from `results/raw/`\n- Group by benchmark category, database, dataset\n- Generate comparison tables\n- Calculate cost estimates\n- Highlight winners/losers\n- Flag known issues\n\nOutput files:\n- `results/BENCHMARK_REPORT.md` - Full summary\n- `results/by-database/{database}.md` - Per-database details\n- `results/by-dataset/{dataset}.md` - Per-dataset comparisons\n- `results/by-category/{category}.md` - OLTP, analytics, graph, etc.\n\nTable format:\n```markdown\n| Database | P50 | P99 | ops/sec | Cost/1M | Notes |\n|----------|-----|-----|---------|---------|-------|\n| db4      | 0.12| 0.45| 8,500   | $0.001  | ✓     |\n| postgres | 0.18| 0.72| 5,600   | $0.001  | WASM  |\n```","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T06:15:53.505184-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:35:17.177909-06:00","closed_at":"2026-01-21T12:35:17.177909-06:00","close_reason":"Report generator complete with summary, per-database, and per-dataset reports with Mermaid charts","dependencies":[{"issue_id":"bench-9zg.2","depends_on_id":"bench-9zg","type":"parent-child","created_at":"2026-01-21T06:15:53.506144-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-9zg.3","title":"Run OLTP benchmarks across all databases","description":"Run OLTP benchmark suite against compatible databases.\n\nDatabases: db4, evodb, postgres, sqlite, @db4/mongo, @dotdo/mongodb, sdb\nDatasets: ecommerce, saas, social (at 1mb, 10mb, 100mb, 1gb tiers)\n\nBenchmarks:\n- Point lookup by PK\n- Range scan (10, 100, 1000 rows)\n- Single insert\n- Batch insert (100, 1000 rows)\n- Single update\n- Batch update\n- Delete\n- Transaction (read-modify-write)\n\nRun command: `pnpm bench:oltp --output-jsonl`","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T06:15:55.115287-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T18:33:04.380698-06:00","closed_at":"2026-01-21T18:33:04.380698-06:00","close_reason":"Worker complete: bench-oltp.ts (1435 lines) for 7 databases, 7 operations, 3 datasets, 4 sizes","dependencies":[{"issue_id":"bench-9zg.3","depends_on_id":"bench-9zg","type":"parent-child","created_at":"2026-01-21T06:15:55.115877-06:00","created_by":"Nathan Clevenger"},{"issue_id":"bench-9zg.3","depends_on_id":"bench-4gk.3","type":"blocks","created_at":"2026-01-21T12:17:44.753888-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-9zg.4","title":"Run analytics benchmarks (ClickBench suite)","description":"Run ClickBench 43 queries against analytics databases.\n\nDatabases: postgres, sqlite, duckdb\nDataset: ClickBench hits table (99M rows)\n\nQueries: All 43 official ClickBench queries\n- Q1-Q10: Simple aggregations\n- Q11-Q20: GROUP BY queries\n- Q21-Q30: Complex filters\n- Q31-Q43: JOINs and window functions\n\nCompare with official ClickBench results.\nOutput per-query timings in JSONL.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T06:15:56.820249-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T18:36:27.042134-06:00","closed_at":"2026-01-21T18:36:27.042134-06:00","close_reason":"Worker complete: bench-analytics.ts (1301 lines) with full ClickBench 43 queries for DuckDB/PostgreSQL/SQLite","external_ref":"gh-10","dependencies":[{"issue_id":"bench-9zg.4","depends_on_id":"bench-9zg","type":"parent-child","created_at":"2026-01-21T06:15:56.820905-06:00","created_by":"Nathan Clevenger"},{"issue_id":"bench-9zg.4","depends_on_id":"bench-4gk.4","type":"blocks","created_at":"2026-01-21T12:17:45.770274-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-9zg.5","title":"Run graph benchmarks (GraphDB, SDB, db4)","description":"Run graph traversal benchmarks.\n\nDatabases: graphdb, sdb, db4\nDatasets: social network, O*NET occupation graph, Wikidata subset\n\nBenchmarks:\n- 1-hop traversal (friends, skills, related entities)\n- 2-hop traversal (friends of friends)\n- Path queries (A -\u003e B with constraints)\n- Reverse traversal (who links to X?)\n- Pattern matching (find all Type with property=value)\n- Batch entity retrieval (100 entities)\n\nOutput traversal depth vs latency curves.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T06:15:58.173472-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T18:33:42.765329-06:00","closed_at":"2026-01-21T18:33:42.765329-06:00","close_reason":"Worker complete: bench-graph.ts for GraphDB/SDB/db4 with 8 graph operations, 3 datasets","external_ref":"gh-11","dependencies":[{"issue_id":"bench-9zg.5","depends_on_id":"bench-9zg","type":"parent-child","created_at":"2026-01-21T06:15:58.174078-06:00","created_by":"Nathan Clevenger"},{"issue_id":"bench-9zg.5","depends_on_id":"bench-4gk.4","type":"blocks","created_at":"2026-01-21T12:17:46.593801-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-9zg.6","title":"Run financial benchmarks (TigerBeetle)","description":"Run financial/accounting benchmarks.\n\nDatabase: TigerBeetle WASM\nDatasets: banking, ecommerce-payments, general-ledger\n\nBenchmarks:\n- Account creation throughput (target: 10K/sec)\n- Transfer processing TPS (target: 100K/sec)\n- Balance lookups\n- Two-phase commit transfers\n- Batch operations (8190 batch size limit)\n- Account history queries\n\nCompare with in-memory db4/sqlite implementations.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T06:15:59.534559-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T18:27:02.160142-06:00","closed_at":"2026-01-21T18:27:02.160142-06:00","close_reason":"Worker created: bench-financial.ts with TigerBeetle DO, 9 benchmark operations, R2 results storage","external_ref":"gh-12","dependencies":[{"issue_id":"bench-9zg.6","depends_on_id":"bench-9zg","type":"parent-child","created_at":"2026-01-21T06:15:59.535174-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-9zg.7","title":"Document known issues and limitations per database","description":"Create `KNOWN_ISSUES.md` documenting database-specific limitations.\n\nPer database, document:\n- Missing SQL features (CTEs, window functions, etc.)\n- Query limitations (no JOINs, max result size, etc.)\n- Type limitations (bigint, JSON, arrays, etc.)\n- Performance gotchas (cold start, memory limits)\n- Cost considerations (blob read/write patterns)\n- WASM-specific issues (bundle size, load time)\n\nFormat:\n```markdown\n## db4\n- ✗ No SQL support (document store only)\n- ✗ No JOINs (must denormalize or multi-query)\n- ✓ Native JSON/document support\n- ⚠ 2MB blob limit per write\n\n## postgres (PGlite WASM)\n- ✓ Full SQL support\n- ✓ JOINs, CTEs, window functions\n- ⚠ 13-14 MB WASM bundle\n- ⚠ Cold start ~500ms\n```","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T06:16:00.918084-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:31:15.527601-06:00","closed_at":"2026-01-21T12:31:15.527601-06:00","close_reason":"Documentation complete: docs/database-limitations.md (660 lines)","external_ref":"gh-13","dependencies":[{"issue_id":"bench-9zg.7","depends_on_id":"bench-9zg","type":"parent-child","created_at":"2026-01-21T06:16:00.918699-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-9zg.8","title":"Run document/MongoDB benchmarks across all implementations","description":"Benchmark all three MongoDB-compatible implementations:\n- @db4/mongo (pure TypeScript)\n- @dotdo/mongodb (PostgreSQL backend)\n- mongo-clickhouse (ClickHouse backend)\n\nDatasets: ecommerce, saas, social (1mb, 10mb, 100mb, 1gb)\n\nBenchmarks:\n- find by _id (point lookup)\n- find with query filter\n- find with projection\n- insertOne / insertMany (100, 1000)\n- updateOne / updateMany\n- deleteOne / deleteMany  \n- aggregate pipeline (group, match, sort, limit)\n- $lookup (join simulation)\n\nCompare cold start, warm query, and throughput across implementations.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:17:33.25566-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T18:32:05.702438-06:00","closed_at":"2026-01-21T18:32:05.702438-06:00","close_reason":"Worker complete: bench-mongodb.ts (1215 lines) comparing db4/postgres/clickhouse MongoDB implementations","dependencies":[{"issue_id":"bench-9zg.8","depends_on_id":"bench-9zg","type":"parent-child","created_at":"2026-01-21T12:17:33.256395-06:00","created_by":"Nathan Clevenger"},{"issue_id":"bench-9zg.8","depends_on_id":"bench-4gk.3","type":"blocks","created_at":"2026-01-21T12:17:47.450868-06:00","created_by":"Nathan Clevenger"},{"issue_id":"bench-9zg.8","depends_on_id":"bench-dqu","type":"blocks","created_at":"2026-01-21T12:17:48.241675-06:00","created_by":"Nathan Clevenger"}]}
{"id":"bench-dqu","title":"Add mongo-clickhouse adapter","description":"Create MongoDB-compatible adapter backed by ClickHouse.\n\nThird MongoDB implementation alongside:\n- @db4/mongo (pure TypeScript, zero WASM)\n- @dotdo/mongodb (PostgreSQL/PGLite backend)\n- NEW: mongo-clickhouse (ClickHouse OLAP backend)\n\nUse cases:\n- Analytics workloads with MongoDB API\n- Time-series aggregations\n- Column-oriented storage for large scans\n\nImplement standard MongoDB operations:\n- find, findOne with projection\n- insertOne, insertMany\n- updateOne, updateMany  \n- deleteOne, deleteMany\n- aggregate pipeline (map to ClickHouse SQL)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:17:32.557111-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:34:50.660901-06:00","closed_at":"2026-01-21T12:34:50.660901-06:00","close_reason":"mongo-clickhouse adapter complete: 1248 lines with full CRUD, aggregation pipeline, and OLAP optimizations"}
{"id":"bench-q91","title":"Deploy and run all benchmark Workers","description":"Deploy staging workers, run benchmarks, collect results, fix issues","status":"open","priority":0,"issue_type":"epic","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T18:49:46.778667-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T18:49:46.778667-06:00"}
