# OLTP Dataset Staging Worker
#
# Generates and stages OLTP datasets directly in-Worker.
# Uses deterministic seeding for reproducible data generation.
# Generated JSONL files are stored in R2 for benchmarking.
#
# Datasets: ecommerce | saas | social | iot
# Sizes: 1mb | 10mb | 100mb | 1gb
#
# Deploy with: wrangler deploy --config worker/stage-oltp.wrangler.toml
#
# @see https://developers.cloudflare.com/workers/platform/environment-variables/
# @see https://developers.cloudflare.com/r2/

name = "stage-oltp"
main = "stage-oltp.ts"
compatibility_date = "2026-01-01"
compatibility_flags = ["nodejs_compat"]

# Workers settings
account_id = "b6641681fe423910342b9ffa1364c76d"

# Build settings
[build]
command = "npx esbuild worker/stage-oltp.ts --bundle --outfile=dist/stage-oltp.js --format=esm --platform=browser --external:cloudflare:workers"

# Performance settings for data generation worker
# Data generation can take significant time for larger datasets
[limits]
cpu_ms = 300000  # 5 minutes max CPU time (Cloudflare limit)

# Observability
[observability]
enabled = true
head_sampling_rate = 1  # Full sampling for staging operations

# =============================================================================
# Durable Objects for Chunked Data Generation
# =============================================================================
#
# The DataGeneratorDO class generates data in chunks to avoid CPU limits.
# Each DO RPC call gets fresh CPU time, allowing large datasets (100mb+)
# to be generated across multiple calls.

[[durable_objects.bindings]]
name = "DATA_GENERATOR"
class_name = "DataGeneratorDO"

[[migrations]]
tag = "v1"
new_classes = ["DataGeneratorDO"]

# =============================================================================
# R2 Bucket for Generated OLTP Datasets
# =============================================================================

[[r2_buckets]]
binding = "DATASETS"
bucket_name = "oltp-datasets"
# preview_bucket_name = "oltp-datasets-preview"

# Bucket structure:
# oltp-datasets/
#   oltp/
#     ecommerce/
#       1mb/
#         customers.jsonl
#         products.jsonl
#         orders.jsonl
#         reviews.jsonl
#       10mb/
#         ...
#       100mb/
#         ...
#       1gb/
#         ...
#     saas/
#       1mb/
#         orgs.jsonl
#         users.jsonl
#         workspaces.jsonl
#         documents.jsonl
#       10mb/
#         ...
#     social/
#       1mb/
#         users.jsonl
#         posts.jsonl
#         comments.jsonl
#         likes.jsonl
#         follows.jsonl
#       10mb/
#         ...
#     iot/
#       1mb/
#         devices.jsonl
#         sensors.jsonl
#         readings.jsonl
#       10mb/
#         ...

# =============================================================================
# Environment Variables
# =============================================================================

[vars]
ENVIRONMENT = "production"
LOG_LEVEL = "info"

# Development environment - uses local/preview buckets
[env.dev]
vars = { ENVIRONMENT = "development", LOG_LEVEL = "debug" }

[[env.dev.r2_buckets]]
binding = "DATASETS"
bucket_name = "oltp-datasets-dev"

# Staging environment - uses staging buckets
[env.staging]
vars = { ENVIRONMENT = "staging", LOG_LEVEL = "info" }

[[env.staging.r2_buckets]]
binding = "DATASETS"
bucket_name = "oltp-datasets-staging"

# =============================================================================
# Deployment Notes
# =============================================================================

# Prerequisites:
#
# 1. Create the R2 bucket:
#    wrangler r2 bucket create oltp-datasets
#
# 2. (Optional) Create dev/staging buckets:
#    wrangler r2 bucket create oltp-datasets-dev
#    wrangler r2 bucket create oltp-datasets-staging
#
# Deployment:
#
# 1. Deploy the worker:
#    wrangler deploy --config worker/stage-oltp.wrangler.toml
#
# 2. Deploy to dev environment:
#    wrangler deploy --config worker/stage-oltp.wrangler.toml --env dev
#
# Usage:
#
# 1. List available datasets:
#    curl "https://stage-oltp.your-subdomain.workers.dev/datasets"
#
# 2. Stage a specific dataset:
#    curl -X POST "https://stage-oltp.your-subdomain.workers.dev/stage/ecommerce/100mb"
#    curl -X POST "https://stage-oltp.your-subdomain.workers.dev/stage/saas/1gb"
#    curl -X POST "https://stage-oltp.your-subdomain.workers.dev/stage/social/10mb"
#    curl -X POST "https://stage-oltp.your-subdomain.workers.dev/stage/iot/100mb"
#
# 3. Check dataset status:
#    curl "https://stage-oltp.your-subdomain.workers.dev/status/ecommerce/100mb"
#
# 4. Delete a staged dataset:
#    curl -X DELETE "https://stage-oltp.your-subdomain.workers.dev/stage/ecommerce/100mb"
#
# 5. Stage all datasets of a specific size:
#    curl -X POST "https://stage-oltp.your-subdomain.workers.dev/stage-all/100mb"
#
# Available Datasets:
#
# - ecommerce: E-commerce OLTP data
#   Tables: customers, products, orders, reviews
#   Use case: Testing e-commerce platforms, order processing
#
# - saas: SaaS Multi-Tenant data
#   Tables: orgs, users, workspaces, documents
#   Use case: Testing multi-tenant SaaS applications
#
# - social: Social Network data
#   Tables: users, posts, comments, likes, follows
#   Use case: Testing social platforms, graph queries
#
# - iot: IoT Timeseries data
#   Tables: devices, sensors, readings
#   Use case: Testing time-series databases, sensor data
#
# Available Sizes:
#
# - 1mb: Minimal dataset for quick testing (~5-10 seconds generation)
# - 10mb: Small dataset for unit tests (~30-60 seconds generation)
# - 100mb: Medium dataset for integration tests (~2-5 minutes generation)
# - 1gb: Large dataset for performance benchmarks (~10-15 minutes generation)
#
# Generation Time Estimates (in-Worker generation):
#
# All data is generated directly in the Worker using deterministic
# seeding (Mulberry32 PRNG) for reproducible results.
#
# Approximate generation times:
# - 1mb:   ~1-5 seconds
# - 10mb:  ~5-30 seconds
# - 100mb: ~30-120 seconds
# - 1gb:   ~2-5 minutes (may hit CPU limits)
#
# Note: Larger datasets (100mb, 1gb) may approach Worker CPU limits.
# Consider generating these in batches or using scheduled workers.
#
# R2 Storage costs:
# - Storage: $0.015/GB/month
# - Total storage for all sizes: ~4-5GB = ~$0.07/month
#
# Health Check:
#
# Verify the worker is running:
#    curl "https://stage-oltp.your-subdomain.workers.dev/health"
#
# Scheduling (optional):
#
# To automatically stage datasets on a schedule, use Cron Triggers:
# [triggers]
# crons = ["0 0 * * 0"]  # Weekly on Sunday at midnight UTC
